# Local Model Checker for Ollama

ğŸ¤– **Check which Ollama AI models your PC can run!**

Live demo: [https://etziya.github.io/localmodelchecker](https://etziya.github.io/localmodelchecker)

## Features

- âœ… Input your PC specs (RAM, VRAM, GPU type)
- ğŸšï¸ Adjust context length with a live slider
- ğŸŸ¢ğŸŸ¡ğŸ”´ Real-time performance indicators (Excellent/Good/Poor/Can't Run)
- ğŸ” Search through 60+ real Ollama models
- ğŸ“Š Visual stats dashboard
- ğŸ’¾ Includes models from 135M to 671B parameters

## Models Included

From tiny edge models to massive datacenter models:
- **Tiny** (135M-1B): SmolLM2, TinyLlama
- **Small** (2-4B): Llama 3.2, Gemma, Phi-3
- **Medium** (7-14B): Mistral, Qwen, DeepSeek-R1
- **Large** (20-40B): Codestral, Mixtral 8x7B
- **Very Large** (70B+): Llama 3.3, DeepSeek-R1
- **Massive** (100B+): GPT-OSS 120B, DeepSeek-V3 671B

## Usage

1. Enter your system specs
2. Move the context length slider
3. Watch the results update in real-time
4. Search for specific models
5. See which models you can run!

## Tech Stack

- Pure HTML/CSS/JavaScript
- No dependencies
- Works offline
- Mobile responsive

## License

MIT License - feel free to use and modify!

---

Built with â¤ï¸ for the Ollama community
